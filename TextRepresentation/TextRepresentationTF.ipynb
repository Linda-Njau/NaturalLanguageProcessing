{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a62b766e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.9.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: absl-py in /home/lindanjau/.local/share/virtualenvs/ai-x-CTmFw2/lib/python3.12/site-packages (from tensorflow-datasets) (2.2.2)\n",
      "Collecting array_record>=0.5.0 (from tensorflow-datasets)\n",
      "  Downloading array_record-0.7.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (899 bytes)\n",
      "Collecting dm-tree (from tensorflow-datasets)\n",
      "  Downloading dm_tree-0.1.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting etils>=1.9.1 (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading etils-1.12.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting immutabledict (from tensorflow-datasets)\n",
      "  Downloading immutabledict-4.2.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy in /home/lindanjau/.local/share/virtualenvs/ai-x-CTmFw2/lib/python3.12/site-packages (from tensorflow-datasets) (2.1.3)\n",
      "Collecting promise (from tensorflow-datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.20 in /home/lindanjau/.local/share/virtualenvs/ai-x-CTmFw2/lib/python3.12/site-packages (from tensorflow-datasets) (5.29.4)\n",
      "Requirement already satisfied: psutil in /home/lindanjau/.local/share/virtualenvs/ai-x-CTmFw2/lib/python3.12/site-packages (from tensorflow-datasets) (7.0.0)\n",
      "Collecting pyarrow (from tensorflow-datasets)\n",
      "  Downloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/lindanjau/.local/share/virtualenvs/ai-x-CTmFw2/lib/python3.12/site-packages (from tensorflow-datasets) (2.32.3)\n",
      "Collecting simple_parsing (from tensorflow-datasets)\n",
      "  Downloading simple_parsing-0.1.7-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets)\n",
      "  Downloading tensorflow_metadata-1.17.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: termcolor in /home/lindanjau/.local/share/virtualenvs/ai-x-CTmFw2/lib/python3.12/site-packages (from tensorflow-datasets) (3.0.1)\n",
      "Collecting toml (from tensorflow-datasets)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: tqdm in /home/lindanjau/.local/share/virtualenvs/ai-x-CTmFw2/lib/python3.12/site-packages (from tensorflow-datasets) (4.67.1)\n",
      "Requirement already satisfied: wrapt in /home/lindanjau/.local/share/virtualenvs/ai-x-CTmFw2/lib/python3.12/site-packages (from tensorflow-datasets) (1.17.2)\n",
      "Collecting einops (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: fsspec in /home/lindanjau/.local/share/virtualenvs/ai-x-CTmFw2/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (2025.3.2)\n",
      "Collecting importlib_resources (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing_extensions in /home/lindanjau/.local/share/virtualenvs/ai-x-CTmFw2/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (4.13.2)\n",
      "Collecting zipp (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/lindanjau/.local/share/virtualenvs/ai-x-CTmFw2/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lindanjau/.local/share/virtualenvs/ai-x-CTmFw2/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/lindanjau/.local/share/virtualenvs/ai-x-CTmFw2/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lindanjau/.local/share/virtualenvs/ai-x-CTmFw2/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow-datasets) (2025.4.26)\n",
      "Collecting attrs>=18.2.0 (from dm-tree->tensorflow-datasets)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: six in /home/lindanjau/.local/share/virtualenvs/ai-x-CTmFw2/lib/python3.12/site-packages (from promise->tensorflow-datasets) (1.17.0)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple_parsing->tensorflow-datasets)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow-datasets)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Downloading tensorflow_datasets-4.9.8-py3-none-any.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m145.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading array_record-0.7.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m94.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading etils-1.12.2-py3-none-any.whl (167 kB)\n",
      "Downloading dm_tree-0.1.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (153 kB)\n",
      "Downloading immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
      "Downloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m564.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading simple_parsing-0.1.7-py3-none-any.whl (112 kB)\n",
      "Downloading tensorflow_metadata-1.17.1-py3-none-any.whl (31 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21580 sha256=af024fdfe150b68fbec9ba2968e3583b0168cc61c9e1701fdd767b62995d19d2\n",
      "  Stored in directory: /home/lindanjau/.cache/pip/wheels/e7/e6/28/864bdfee5339dbd6ddcb5a186286a8e217648ec198bdf0097d\n",
      "Successfully built promise\n",
      "Installing collected packages: zipp, toml, pyarrow, promise, importlib_resources, immutabledict, googleapis-common-protos, etils, einops, docstring-parser, attrs, tensorflow-metadata, simple_parsing, dm-tree, array_record, tensorflow-datasets\n",
      "Successfully installed array_record-0.7.2 attrs-25.3.0 dm-tree-0.1.9 docstring-parser-0.16 einops-0.8.1 etils-1.12.2 googleapis-common-protos-1.70.0 immutabledict-4.2.1 importlib_resources-6.5.2 promise-2.3 pyarrow-20.0.0 simple_parsing-0.1.7 tensorflow-datasets-4.9.8 tensorflow-metadata-1.17.1 toml-0.10.2 zipp-3.21.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b23ca717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 15:53:06.978336: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "WARNING:absl:Variant folder /home/lindanjau/tensorflow_datasets/ag_news_subset/1.0.0 has no dataset_info.json\n",
      "/home/lindanjau/.local/share/virtualenvs/ai-x-CTmFw2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/lindanjau/tensorflow_datasets/ag_news_subset/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:19<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:21<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:22<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:23<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:24<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:25<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:26<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:27<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:29<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:29<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:30<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:31<?, ? url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:31<00:00, 31.26s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:31<00:00, 31.26s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:31<00:00, 31.26s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:31<00:00, 31.26s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:31<00:00, 31.26s/ url]\n",
      "\u001b[A\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:31<00:00, 31.26s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:31<00:00, 31.26s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:31<00:00, 31.26s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:31<00:00, 31.26s/ url]\n",
      "Extraction completed...: 100%|██████████| 4/4 [00:31<00:00,  7.92s/ file]\n",
      "Dl Size...: 100%|██████████| 11/11 [00:31<00:00,  2.88s/ MiB]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:31<00:00, 31.68s/ url]\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset ag_news_subset downloaded and prepared to /home/lindanjau/tensorflow_datasets/ag_news_subset/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0],True)\n",
    "\n",
    "dataset = tfds.load('ag_news_subset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29b8e88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset = 120000\n",
      "Length of test dataset = 7600\n"
     ]
    }
   ],
   "source": [
    "ds_train = dataset['train']\n",
    "ds_test = dataset['test']\n",
    "\n",
    "print(f\"Length of train dataset = {len(ds_train)}\")\n",
    "print(f\"Length of test dataset = {len(ds_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "939ad02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (Sci/Tech) -> b'AMD Debuts Dual-Core Opteron Processor' b'AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.'\n",
      "1 (Sports) -> b\"Wood's Suspension Upheld (Reuters)\" b'Reuters - Major League Baseball\\\\Monday announced a decision on the appeal filed by Chicago Cubs\\\\pitcher Kerry Wood regarding a suspension stemming from an\\\\incident earlier this season.'\n",
      "2 (Business) -> b'Bush reform may have blue states seeing red' b'President Bush #39;s  quot;revenue-neutral quot; tax reform needs losers to balance its winners, and people claiming the federal deduction for state and local taxes may be in administration planners #39; sights, news reports say.'\n",
      "3 (Sci/Tech) -> b\"'Halt science decline in schools'\" b'Britain will run out of leading scientists unless science education is improved, says Professor Colin Pillinger.'\n",
      "1 (Sports) -> b'Gerrard leaves practice' b'London, England (Sports Network) - England midfielder Steven Gerrard injured his groin late in Thursday #39;s training session, but is hopeful he will be ready for Saturday #39;s World Cup qualifier against Austria.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 16:06:04.718385: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:387] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2025-05-16 16:06:04.733782: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "for i,x in zip(range(5),ds_train):\n",
    "    print(f\"{x['label']} ({classes[x['label']]}) -> {x['title']} {x['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc92ec82",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras._tf_keras.keras.layers' has no attribute 'experimental'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m vocab_size = \u001b[32m50000\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m vectorizer = \u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexperimental\u001b[49m.preprocessing.TextVectorization(max_tokens=vocab_size)\n\u001b[32m      3\u001b[39m vectorizer.adapt(ds_train.take(\u001b[32m500\u001b[39m).map(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m]+\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m+x[\u001b[33m'\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m'\u001b[39m]))\n",
      "\u001b[31mAttributeError\u001b[39m: module 'keras._tf_keras.keras.layers' has no attribute 'experimental'"
     ]
    }
   ],
   "source": [
    "vocab_size = 50000\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size)\n",
    "vectorizer.adapt(ds_train.take(500).map(lambda x: x['title']+' '+x['description']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f648ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_vocabulary()\n",
    "vocab_size = len(vocab)\n",
    "print(vocab[:10])\n",
    "print(f\"Length of vocabulary: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer('I love to play with my words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e187cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c11f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-x-CTmFw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
